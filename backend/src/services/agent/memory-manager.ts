/**
 * Memory Manager
 *
 * - æ»‘åŠ¨çª—å£ç®¡ç† â€” ä¿ç•™æœ€è¿‘ N æ¡åŸå§‹æ¶ˆæ¯
 * - å†å²æ‘˜è¦ç”Ÿæˆ â€” å¯¹è¶…å‡ºçª—å£çš„æ¶ˆæ¯ç”Ÿæˆ LLM æ‘˜è¦
 * - ä¸Šä¸‹æ–‡ç»„è£… â€” å°†æ‘˜è¦ + æ»‘åŠ¨çª—å£ç»„åˆä¸º LLM å¯ç”¨çš„æ¶ˆæ¯åºåˆ—
 * - ä»»åŠ¡è®¡åˆ’é›†æˆ â€” å°†æ´»è·ƒçš„ TaskPlan æ³¨å…¥ä¸Šä¸‹æ–‡
 */

import { IMessage } from "../../models/Conversation.model";
import { config } from "../../config/env";
import { logger } from "../../lib/logger";
import {
  LlmMessage,
  MemoryState,
  ConversationSummary,
  TaskPlan,
  MEMORY_SLIDING_WINDOW,
  MEMORY_SUMMARY_THRESHOLD,
  MAX_CONTEXT_CHARS,
  TASK_STATUS,
} from "./agent.types";

const SUMMARY_PROMPT = `You are a conversation summarizer. Given a series of messages from a chat between a user and an AI assistant for a cloud drive platform, create a concise summary that captures:
1. Key user intents and requests
2. Important actions taken (files created, moved, edited, etc.)
3. Any decisions made or preferences expressed
4. Current context (what file/folder the user is working with)

Rules:
- Be concise but preserve critical details
- Include specific file/folder names, IDs, or paths that were discussed
- Preserve any unresolved requests or pending actions
- Output the summary in the same language as the conversation
- Maximum 300 words`;

async function generateSummary(messages: IMessage[]): Promise<string | null> {
  const apiKey = config.llmApiKey;
  const baseUrl = config.llmBaseUrl;
  const model = config.llmModel;

  if (!apiKey) return null;

  const formatted = messages
    .map((m) => {
      let text = `[${m.role}]: ${m.content}`;
      if (m.toolCalls?.length) {
        text += `\n  (Tools used: ${m.toolCalls.map((t) => t.toolName).join(", ")})`;
      }
      return text;
    })
    .join("\n");

  try {
    const response = await fetch(`${baseUrl}/chat/completions`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${apiKey}`,
      },
      body: JSON.stringify({
        model,
        messages: [
          { role: "system", content: SUMMARY_PROMPT },
          {
            role: "user",
            content: `Summarize this conversation:\n\n${formatted}`,
          },
        ],
        temperature: 0.2,
        max_tokens: 500,
      }),
    });

    if (!response.ok) {
      logger.warn({ status: response.status }, "Summary generation failed");
      return null;
    }

    const data = (await response.json()) as {
      choices: Array<{ message: { content: string } }>;
    };

    return data.choices?.[0]?.message?.content?.trim() || null;
  } catch (error) {
    logger.warn({ error }, "Summary generation error");
    return null;
  }
}

export class MemoryManager {
  // ä»å®Œæ•´æ¶ˆæ¯å†å²æ„å»º MemoryState

  // å¦‚æœæ¶ˆæ¯æ•°è¶…è¿‡ MEMORY_SUMMARY_THRESHOLDï¼š
  // 1. å¯¹è¶…å‡ºæ»‘åŠ¨çª—å£çš„æ—§æ¶ˆæ¯ç”Ÿæˆæ‘˜è¦
  // 2. ä¿ç•™æœ€è¿‘ MEMORY_SLIDING_WINDOW æ¡æ¶ˆæ¯
  async buildMemoryState(
    messages: IMessage[],
    existingSummaries: ConversationSummary[] = [],
    activePlan?: TaskPlan,
  ): Promise<MemoryState> {
    const totalCount = messages.length;

    if (totalCount <= MEMORY_SUMMARY_THRESHOLD) {
      return {
        summaries: existingSummaries,
        recentMessages: messages,
        activePlan,
        totalMessageCount: totalCount,
      };
    }

    // åˆ†ç¦»æ—§æ¶ˆæ¯å’Œæ–°æ¶ˆæ¯
    const cutoff = totalCount - MEMORY_SLIDING_WINDOW;
    const recentMessages = messages.slice(cutoff);

    // æ£€æŸ¥æ˜¯å¦éœ€è¦æ–°çš„æ‘˜è¦
    const lastSummarizedIdx =
      existingSummaries.length > 0
        ? existingSummaries[existingSummaries.length - 1].messageRange.to
        : 0;

    const newSummaries = [...existingSummaries];

    if (lastSummarizedIdx < cutoff) {
      // æœ‰æ–°çš„æœªæ‘˜è¦æ¶ˆæ¯
      const unsummarized = messages.slice(lastSummarizedIdx, cutoff);
      if (unsummarized.length > 0) {
        const summaryText = await generateSummary(unsummarized);
        if (summaryText) {
          newSummaries.push({
            summary: summaryText,
            messageRange: { from: lastSummarizedIdx, to: cutoff },
            createdAt: new Date(),
          });

          logger.info(
            {
              range: `${lastSummarizedIdx}-${cutoff}`,
              summaryLength: summaryText.length,
            },
            "Generated conversation summary",
          );
        }
      }
    }

    return {
      summaries: newSummaries,
      recentMessages,
      activePlan,
      totalMessageCount: totalCount,
    };
  }

  // å°† MemoryState ç»„è£…ä¸º LLM æ¶ˆæ¯åºåˆ—

  // ç»“æ„ï¼š
  //   [system prompt]
  //   [summary context]   if has summaries
  //   [task plan context] if has active plan
  //   [recent messages]   if has recent messages
  assembleLlmMessages(
    systemPrompt: string,
    memoryState: MemoryState,
  ): LlmMessage[] {
    const messages: LlmMessage[] = [{ role: "system", content: systemPrompt }];

    // æ³¨å…¥æ‘˜è¦ä¸Šä¸‹æ–‡
    if (memoryState.summaries.length > 0) {
      const summaryBlock = memoryState.summaries
        .map(
          (s, i) =>
            `[Summary ${i + 1} (msgs ${s.messageRange.from}-${s.messageRange.to})]: ${s.summary}`,
        )
        .join("\n\n");

      messages.push({
        role: "system",
        content: `## Conversation History Summary\nThe following is a summary of earlier messages in this conversation:\n\n${summaryBlock}\n\n---\nRecent messages follow below.`,
      });
    }

    // æ³¨å…¥ä»»åŠ¡è®¡åˆ’
    if (memoryState.activePlan && !memoryState.activePlan.isComplete) {
      const planBlock = this.formatTaskPlan(memoryState.activePlan);
      messages.push({
        role: "system",
        content: `## Active Task Plan\n${planBlock}`,
      });
    }

    // æ³¨å…¥æ»‘åŠ¨çª—å£æ¶ˆæ¯
    for (const msg of memoryState.recentMessages) {
      if (msg.role === "user") {
        messages.push({ role: "user", content: msg.content });
      } else if (msg.role === "assistant") {
        messages.push({ role: "assistant", content: msg.content });
      }
    }

    // ä¸Šä¸‹æ–‡çª—å£ä¿æŠ¤
    this.compressIfNeeded(messages);

    return messages;
  }

  // Router è½»é‡æ‘˜è¦ï¼šè¦å–æœ€è¿‘ä¸€ä¸ªæ‘˜è¦ï¼ˆå‰æ–‡è¯­ä¹‰èƒŒæ™¯ï¼‰+ æœ€è¿‘å‡ æ¡ç”¨æˆ·æ¶ˆæ¯ï¼ˆæœ€æ–°éœ€æ±‚ï¼‰
  getRouterContext(memoryState: MemoryState): string | undefined {
    const parts: string[] = [];

    if (memoryState.summaries.length > 0) {
      const lastSummary =
        memoryState.summaries[memoryState.summaries.length - 1];
      parts.push(`Previous context: ${lastSummary.summary.slice(0, 200)}`);
    }

    const recentUserMsgs = memoryState.recentMessages
      .filter((m) => m.role === "user")
      .slice(-3)
      .map((m) => m.content.slice(0, 100));

    if (recentUserMsgs.length > 0) {
      parts.push(`Recent user messages: ${recentUserMsgs.join(" | ")}`);
    }

    return parts.length > 0 ? parts.join("\n") : undefined;
  }

  private formatTaskPlan(plan: TaskPlan): string {
    const lines = [`**Goal**: ${plan.goal}`];
    lines.push(
      `**Progress**: Step ${plan.currentStep} of ${plan.steps.length}`,
    );
    lines.push("");

    for (const step of plan.steps) {
      const statusIcon =
        step.status === TASK_STATUS.COMPLETED
          ? "âœ…"
          : step.status === TASK_STATUS.IN_PROGRESS
            ? "ğŸ”„"
            : step.status === TASK_STATUS.FAILED
              ? "âŒ"
              : step.status === TASK_STATUS.SKIPPED
                ? "â­ï¸"
                : "â¬œ";

      let line = `${statusIcon} Step ${step.id}: ${step.title}`;
      if (step.result) line += ` â€” ${step.result.slice(0, 80)}`;
      if (step.error) line += ` â€” Error: ${step.error.slice(0, 80)}`;
      lines.push(line);
    }

    if (plan.summary) {
      lines.push("");
      lines.push(`**Summary so far**: ${plan.summary}`);
    }

    return lines.join("\n");
  }

  private estimateChars(messages: LlmMessage[]): number {
    let total = 0;
    for (const msg of messages) {
      if (msg.content) total += msg.content.length;
      if (msg.tool_calls) {
        for (const tc of msg.tool_calls) {
          total += tc.function.arguments.length + tc.function.name.length;
        }
      }
    }
    return total;
  }

  compressIfNeeded(messages: LlmMessage[]): void {
    let totalChars = this.estimateChars(messages);
    if (totalChars <= MAX_CONTEXT_CHARS) return;

    logger.info(
      { totalChars, limit: MAX_CONTEXT_CHARS },
      "Memory manager: compressing context",
    );

    // å…ˆå‹ç¼© tool ç»“æœ
    const KEEP_RECENT = 6;
    const shrinkBound = Math.max(1, messages.length - KEEP_RECENT);

    for (let i = 1; i < shrinkBound && totalChars > MAX_CONTEXT_CHARS; i++) {
      const msg = messages[i];
      if (msg.role === "tool" && msg.content && msg.content.length > 200) {
        const oldLen = msg.content.length;
        msg.content =
          msg.content.slice(0, 150) +
          `\n[...compressed â€” original ${oldLen} chars]`;
        totalChars -= oldLen - msg.content.length;
      }
    }

    if (totalChars <= MAX_CONTEXT_CHARS) return;

    // å†åˆ é™¤æ—§æ¶ˆæ¯
    while (
      messages.length > KEEP_RECENT + 1 &&
      totalChars > MAX_CONTEXT_CHARS
    ) {
      const removed = messages.splice(1, 1)[0];
      if (removed.content) totalChars -= removed.content.length;
    }

    logger.info(
      { newTotalChars: totalChars, messageCount: messages.length },
      "Memory manager: context compressed",
    );
  }
}
